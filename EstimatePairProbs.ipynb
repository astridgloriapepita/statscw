{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the probabilities of successive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the usual things\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Also stuff for reading files and processing strings\n",
    "import string\n",
    "import collections\n",
    "from pathlib import Path \n",
    "\n",
    "# And, finally, stuff to write dictionaries out to files\n",
    "# and read them back again.\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to estimate the probabilities $p(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I analyse a large body of training text to estimate the probability $p(a,b)$, which is the probability that symbol $a$ is followed by symbol $b$. As a symbol $a$ has to be followed by *something*, it follows that $\\left( \\sum_{b} p(a,b) \\right) = 1$.\n",
    "\n",
    "I'll estimate the $p(a,b)$ in a Bayesian way, using a calculation modelled on that in Section 5.2.1.7, entitled *Smoothing*, in\n",
    "> S. Rogers & M. Girolami (2016), *A First Course in Machine Learning*, 2nd edition, Chapman & Hall/CRC. ISBN 9781498738484\n",
    "\n",
    "A key feature of their approach is that yields a non-zero, though perhaps very small, probability for all pairs $ab$, even those that never appear in the traning text. This ameliorates a problem produced by the finiteness of the training data. Suppose that, in the training text, it never happens that an `n` is followed by a `w`. The simplest approach to estimating $p(n,w)$ would then yield $p(n,w) = 0$ and if we later tried to decode a message that contained the word `unwind`, we'd end up assigning a likelihood of zero to the correct cypher, purely becasue we'd underestimated $p(n,w)$.\n",
    "\n",
    "From a technical point of view, we are going to do Bayesian inference for the $p(a,b)$ with a [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) likelihood for the observed counts of successive letter pairs and a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) prior on the probabilities. Because these distributions are [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior), we'll end up with a Dirichlet posterior as well, and we'll then make a maximum-*a-posteriori* (MAP) estimate for the $p(a,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to standardise the text by making it all lowercase and replacing characters that aren't in the allowed alphabet with blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where would heavy metal be without the *mlaut*'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardiseText( rawText, allowedChars, replacementChar=' ' ):\n",
    "    # Make all the characters lower case\n",
    "    rawText = rawText.lower()\n",
    "\n",
    "    # Replace any characters that aren't part of our list\n",
    "    # of allowed characters with the replacement character\n",
    "    standardisedText = \"\"\n",
    "    for char in rawText:\n",
    "        if allowedChars.find(char) == -1:\n",
    "            # char isn't one of the allowed ones\n",
    "            standardisedText = standardisedText + replacementChar\n",
    "        else:\n",
    "            standardisedText = standardisedText + char\n",
    "            \n",
    "    return( standardisedText )\n",
    "\n",
    "# Do a small test\n",
    "testText = \"Where would heavy metal be without the Ã¼mlaut?\"\n",
    "standardiseText( testText, ' abcdefghijklmnopqrstuvwxyz', '*' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main event: slurp the training text, count pairs and estimate $p(a,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the bulk of the work of estimating the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLogPairProbs( trainingTextFiles, allowedChars ):\n",
    "    ############################################\n",
    "    # Read and standardise the training texts\n",
    "    ############################################\n",
    "    # Read the whole training text into a single string \n",
    "    # See https://stackoverflow.com/questions/1631897/python-file-slurp\n",
    "    rawTrainingText = ''\n",
    "    for textFile in trainingTextFiles:\n",
    "        rawTrainingText += Path(textFile).read_text()\n",
    "        \n",
    "    print( 'The training text is ' + str(len(rawTrainingText)) + ' characters long.')\n",
    "    \n",
    "    ###########################################\n",
    "    # Standardise the training text\n",
    "    ###########################################\n",
    "    # Make sure the list of allowed characters\n",
    "    # is sorted into a standard order and doesn't \n",
    "    # contain any repeats.\n",
    "    charSet = sorted( list( set(allowedChars) ) ) \n",
    "    charSetStr = ''.join(charSet)\n",
    "    nSymbols = len(charSetStr)\n",
    "    \n",
    "    # Standardise the text\n",
    "    stdTrainingText = standardiseText( rawTrainingText, charSetStr, ' ' )        \n",
    "    \n",
    "    ##########################################\n",
    "    # Count appearances of pairs\n",
    "    ##########################################\n",
    "    # Build a dictionary whose keys are the allowed characters\n",
    "    # and whose values are integers. They'll eventually be counts.\n",
    "    emptyCountDict = dict.fromkeys( charSet, 0 )\n",
    "\n",
    "    # Build a dictionary whose keys are the allowed characters \n",
    "    # and whose values are copies of the empty count dictionary\n",
    "    pairCounts = dict.fromkeys( charSet )\n",
    "    for char in pairCounts.keys():\n",
    "        pairCounts[char] = emptyCountDict.copy()\n",
    "\n",
    "    # Now count appearances of pairs\n",
    "    for j in range(1, len(stdTrainingText)):\n",
    "        firstChar = stdTrainingText[j-1]\n",
    "        secondChar = stdTrainingText[j]\n",
    "        pairCounts[firstChar][secondChar] += 1\n",
    "        \n",
    "    ###########################################\n",
    "    # Estimate the probabilities, or rather, \n",
    "    # their logs\n",
    "    ###########################################\n",
    "    # Build a dictionary-of-dictionaries that holds\n",
    "    # the parameters of the Dirichlet posteriors.\n",
    "    priorAlpha = 2 # favours broadly uniform, nonzero probabilities for all letter pairs\n",
    "\n",
    "    dirichletPosterior = dict.fromkeys( charSet )\n",
    "    for firstChar in dirichletPosterior.keys():\n",
    "        dirichletPosterior[firstChar] = dict.fromkeys( charSet )\n",
    "        for secondChar in dirichletPosterior[firstChar].keys():\n",
    "            dirichletPosterior[firstChar][secondChar] = pairCounts[firstChar][secondChar] + priorAlpha\n",
    "            \n",
    "    # Get the MAP estimates for the probabilities\n",
    "    logPairProbs = dict.fromkeys( charSet )\n",
    "    for firstChar in logPairProbs.keys():\n",
    "        # Add up all the entries in the posterior for this row\n",
    "        posteriorAlphaSum = 0.0\n",
    "        for secondChar in dirichletPosterior[firstChar].keys():\n",
    "            posteriorAlphaSum += dirichletPosterior[firstChar][secondChar]\n",
    "\n",
    "        # Initialise the result\n",
    "        logPairProbs[firstChar] = dict.fromkeys( charSet, 0.0 )\n",
    "\n",
    "        # Get the logs of the maximum-a-posteriori estimates por the probs\n",
    "        logNomalisation = math.log( posteriorAlphaSum - len(dirichletPosterior[firstChar].keys()) )\n",
    "        for secondChar in logPairProbs[firstChar].keys():\n",
    "            logPairProbs[firstChar][secondChar] = math.log(dirichletPosterior[firstChar][secondChar] - 1) - logNomalisation\n",
    "    \n",
    "    return( logPairProbs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the tools and estimating $p(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the tools of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training text is 2505573 characters long.\n"
     ]
    }
   ],
   "source": [
    "trainingTextFiles = ['CaoJoly_DreamOfTheRedChamber_Vol1.txt', 'CaoJoly_DreamOfTheRedChamber_Vol2.txt']\n",
    "commonChars = \"abcdefghijklmnopqrstuvwxyz0123456789 ,.?!:;\"\n",
    "logPairProbs = findLogPairProbs( trainingTextFiles, commonChars )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, assemble the results into a matrix and check that the rows sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nSymbols = len(logPairProbs.keys())\n",
    "probMat = np.zeros( (nSymbols, nSymbols) )\n",
    "\n",
    "row = 0\n",
    "for firstChar in logPairProbs.keys():\n",
    "    probMat[row,:] = np.exp( np.array(list(logPairProbs[firstChar].values())))\n",
    "    row += 1\n",
    "    \n",
    "np.sum( probMat, axis=1 ) # sums across rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the logs of the $p(a,b)$ to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write the log dictionary out and then read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary out to a file\n",
    "pairProbFile = \"LogPairProbDict.json\"\n",
    "jsonForProbDict = json.dumps(logPairProbs)\n",
    "myFileObj = open(pairProbFile, \"w\")\n",
    "myFileObj.write(jsonForProbDict)\n",
    "myFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read it back \n",
    "myFileObj = open(pairProbFile, \"r\")\n",
    "jsonFromFile = myFileObj.read()\n",
    "dictFromFile = json.loads(jsonFromFile)\n",
    "myFileObj.close()\n",
    "\n",
    "# Check whether the two versions agree\n",
    "dictFromFile == logPairProbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
